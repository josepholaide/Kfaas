{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtzs4x06tv0O"
   },
   "source": [
    "# Install relevant libraries\n",
    "\n",
    "\n",
    "1.   update pip\n",
    "2.   install kubeflow sdk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "38y13drotnXK",
    "outputId": "61184254-57cb-4f29-c0e5-dba30df0c914",
    "tags": [
     "imports"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (21.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6a8V8LN9ttJT"
   },
   "outputs": [],
   "source": [
    "!pip install kfp --upgrade --user --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/KfaaS/kale'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "cd to home directory or directory of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EnlKV9mHtnXQ",
    "outputId": "fc5cf010-a193-4e64-c725-2bb09807b80c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "cd '/home/jovyan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "create folder to store outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZSERKXIHtnXR",
    "outputId": "c0b806f4-01a9-4271-ad5f-3541fd57c3cc"
   },
   "outputs": [],
   "source": [
    "mkdir 'datastore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "taqx2u69tnXS",
    "outputId": "32ec75b1-7d1e-434e-8834-aa841fecd4d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kfp\n",
      "Version: 1.8.11\n",
      "Summary: KubeFlow Pipelines SDK\n",
      "Home-page: https://github.com/kubeflow/pipelines\n",
      "Author: The Kubeflow Authors\n",
      "Author-email: \n",
      "License: UNKNOWN\n",
      "Location: /home/jovyan/.local/lib/python3.6/site-packages\n",
      "Requires: absl-py, click, cloudpickle, dataclasses, Deprecated, docstring-parser, fire, google-api-python-client, google-auth, google-cloud-storage, jsonschema, kfp-pipeline-spec, kfp-server-api, kubernetes, protobuf, pydantic, PyYAML, requests-toolbelt, strip-hints, tabulate, typer, typing-extensions, uritemplate\n",
      "Required-by: kubeflow-kale\n"
     ]
    }
   ],
   "source": [
    "# You may need to restart your notebook kernel after updating the kfp sdk\n",
    "! pip show kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4B2CjdRcuRgT"
   },
   "source": [
    "## Import kubeflow pipeline libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_8P4-rCDtnXT"
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hrbGfqDZtnXU"
   },
   "outputs": [],
   "source": [
    "# create  directory for outputs.\n",
    "output_dir = \"/home/jovyan/datastore\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWPLyw_DuzSl"
   },
   "source": [
    "## Kubeflow pipeline component creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98g9LoIcuaPB"
   },
   "source": [
    "step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "lGK3hlXdtnXV"
   },
   "outputs": [],
   "source": [
    "# download data step\n",
    "\n",
    "def download_data(data_path):\n",
    "    import zipfile\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"wget\"])\n",
    "    \n",
    "    import wget\n",
    "    # download files\n",
    "    wget.download('https://github.com/josepholaide/KfaaS/blob/main/kale/data/train.csv.zip?raw=true', f'{data_path}/train_csv.zip')\n",
    "    wget.download('https://github.com/josepholaide/KfaaS/blob/main/kale/data/test.csv.zip?raw=true', f'{data_path}/test_csv.zip')\n",
    "    \n",
    "    with zipfile.ZipFile(f\"{data_path}/train_csv.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "        \n",
    "    with zipfile.ZipFile(f\"{data_path}/test_csv.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XyAH8LvZtnXW",
    "outputId": "6569212a-b21b-43fc-baab-0acfd70ce6af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "download_data(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9Yhdk3xudcn"
   },
   "source": [
    "step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oEdsQpH2tnXX"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "def load_data(data_path,train_data,test_data):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn'])\n",
    "    # import Library\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "    #importing the data\n",
    "    # Data Path\n",
    "    train_data_path = data_path + '/train.csv'\n",
    "    test_data_path = data_path + '/test.csv'\n",
    "\n",
    "    # Loading dataset into pandas \n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    \n",
    "    # split features and label\n",
    "    X = train_df.drop('label', axis=1)\n",
    "    y = train_df.label\n",
    "    \n",
    "    # Reshape image in 3 dimensions (height = 28px, width = 28px , channel = 1)\n",
    "    X = X.values.reshape(-1,28,28,1)\n",
    "\n",
    "    # Normalize the data\n",
    "    X = X / 255.0\n",
    "    \n",
    "    # split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    #Save the train_data as a pickle file to be used by the train component.\n",
    "    with open(f'{data_path}/{train_data}', 'wb') as f:\n",
    "        pickle.dump((X_train,  y_train), f)\n",
    "        \n",
    "    #Save the test_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/{test_data}', 'wb') as f:\n",
    "        pickle.dump((X_test,  y_test), f)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "U3OeG1dytnXY",
    "outputId": "9f92e0b7-4ca6-4234-8202-f170cf10df8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "load_data(output_dir,'train_data','test_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqqSNS-OugYq"
   },
   "source": [
    "step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8KGMGMEmtnXZ"
   },
   "outputs": [],
   "source": [
    "def train(data_path,train_data,model_path):\n",
    "    import pickle\n",
    "    # import Library\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','tensorflow'])\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras import layers\n",
    "\n",
    "    #loading the train data\n",
    "    with open(f'{data_path}/{train_data}', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "        \n",
    "    # Separate the X_train from y_train.\n",
    "    X_train, y_train = train_data\n",
    "    \n",
    "    #initializing the classifier model with its input, hidden and output layers\n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(filters = 56, kernel_size = (5,5), activation ='relu'))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Conv2D(filters = 100, kernel_size = (3,3), activation ='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv2D(filters = 100, kernel_size = (3,3), activation ='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "    #Compiling the classifier model with Stochastic Gradient Desecnt\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # model fitting\n",
    "    model.fit(X_train, y_train,\n",
    "              epochs=2,\n",
    "              batch_size=64)\n",
    " \n",
    "    #saving the model\n",
    "    model.save(f'{data_path}/{model_path}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "QPDHgayetnXa",
    "outputId": "8600b218-8759-46fd-ebdc-9501ccbe3d1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "591/591 [==============================] - 70s 119ms/step - loss: 0.1630 - accuracy: 0.9504\n",
      "Epoch 2/2\n",
      "591/591 [==============================] - 68s 116ms/step - loss: 0.0545 - accuracy: 0.9837\n",
      "WARNING:tensorflow:From /home/jovyan/.local/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/jovyan/.local/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/datastore/model/assets\n"
     ]
    }
   ],
   "source": [
    "train(output_dir,\"train_data\",\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwXJuoHQui3d"
   },
   "source": [
    "step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "i88IyZN8tnXb"
   },
   "outputs": [],
   "source": [
    "def predict(data_path,test_data,model):\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','tensorflow'])\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import load_model\n",
    "    \n",
    "    #loading the X_test and y_test\n",
    "    with open(f'{data_path}/{test_data}', 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    # Separate the X_test from y_test.\n",
    "    X_test, y_test = test_data\n",
    "    #loading the model\n",
    "    model = load_model(f'{data_path}/{model}')\n",
    "\n",
    "    #Evaluate the model and print the results\n",
    "    test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=0)\n",
    "    \n",
    "    #model's prediction on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # convert predictions to positional indices with max values\n",
    "    y_pred_class = np.argmax(y_pred,axis = 1)\n",
    "\n",
    "    #saving the test_loss and test_acc\n",
    "    with open(f'{data_path}/performance.txt', 'w') as f:\n",
    "        f.write(\"Test_loss: {}, Test_accuracy: {} \".format(test_loss,test_acc))\n",
    "        \n",
    "    #saving the predictions\n",
    "    with open(f'{data_path}/results.txt', 'w') as result:\n",
    "        result.write(\" Prediction: {}, Actual: {} \".format(y_pred_class,y_test))\n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "wHIp17sxtnXb",
    "outputId": "ce2c74ed-76d3-42bf-f013-348f91fe416b",
    "tags": [
     "block:display_result",
     "prev:modelling"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "predict(output_dir,\"test_data\",\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "create kubeflow pipeline components from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2vCb1IgGtnXc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create light weight components\n",
    "download_op = comp.create_component_from_func(download_data,base_image=\"python:3.7.1\")\n",
    "load_op = comp.create_component_from_func(load_data,base_image=\"python:3.7.1\")\n",
    "train_op = comp.create_component_from_func(train, base_image=\"tensorflow/tensorflow:latest\")\n",
    "predict_op = comp.create_component_from_func(predict, base_image=\"tensorflow/tensorflow:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3bNFpBOuuG-"
   },
   "source": [
    "## Kubeflow pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "LVbUms_ptnXc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create client that would enable communication with the Pipelines API server \n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ECZRaIgCtnXd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "@dsl.pipeline(name=\"digit recognizer Pipeline\", description=\"Performs Preprocessing, training and prediction of digits\")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def digit_recognize_pipeline(data_path: str,\n",
    "                             train_data: str,\n",
    "                             test_data:str,\n",
    "                             model_path:str):\n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    volume_op = dsl.VolumeOp(\n",
    "    name=\"data_volume\",\n",
    "    resource_name=\"data-volume\",\n",
    "    size=\"1Gi\",\n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "\n",
    "    # Create preprocess components.\n",
    "    download_container = download_op(data_path).add_pvolumes({data_path: volume_op.volume})\n",
    "    # Create train component.\n",
    "    load_container = load_op(data_path,train_data,test_data).add_pvolumes({data_path: download_container.pvolume})\n",
    "    # Create prediction component.\n",
    "    train_container = train_op(data_path, train_data, model_path).add_pvolumes({data_path: load_container.pvolume})\n",
    "    # Create prediction component.\n",
    "    predict_container = predict_op(data_path, test_data, model_path).add_pvolumes({data_path: train_container.pvolume})\n",
    "    \n",
    "    # Print the result of the prediction (step 5)\n",
    "    result_container = dsl.ContainerOp(\n",
    "        name=\"print_prediction\",\n",
    "        image='library/bash:4.4.23',\n",
    "        pvolumes={data_path: predict_container.pvolume},\n",
    "        arguments=['cat', f'{data_path}/results.txt']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Jq2M3chhtnXd",
    "outputId": "cd75c395-f0d1-415f-8205-9ea23d52fdb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/kfp/dsl/_container_op.py:1264: FutureWarning: Please create reusable components instead of constructing ContainerOp instances directly. Reusable components are shareable, portable and have compatibility and support guarantees. Please see the documentation: https://www.kubeflow.org/docs/pipelines/sdk/component-development/#writing-your-component-definition-file The components can be created manually (or, in case of python, using kfp.components.create_component_from_func or func_to_container_op) and then loaded using kfp.components.load_component_from_file, load_component_from_uri or load_component_from_text: https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html#kfp.components.load_component_from_file\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/6f6c9b81-54e3-414b-974a-6fe8b445a59e\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/7e425767-30f4-4c07-b961-518b72fe1086\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH = \"/mnt\"\n",
    "TRAIN_DATA = \"train_data\"\n",
    "TEST_DATA = \"test_data\"\n",
    "MODEL_FILE= \"model.h5\"\n",
    "\n",
    "\n",
    "pipeline_func = digit_recognize_pipeline\n",
    "\n",
    "experiment_name = 'digit_recognize_lightweight'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH,\n",
    "            \"train_data\": TRAIN_DATA,\n",
    "            \"test_data\": TEST_DATA,\n",
    "            \"model_path\":MODEL_FILE}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6TIs2N5tnXe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "kubepipe.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "docker_image": "gcr.io/arrikto/jupyter-kale-py36@sha256:dd3f92ca66b46d247e4b9b6a9d84ffbb368646263c2e3909473c3b851f3fe198",
   "experiment": {
    "id": "new",
    "name": "pipe"
   },
   "experiment_name": "pipe",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "poppww",
   "snapshot_volumes": true,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:access-rok:true"
   ],
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "demoo-workspace-fcx9q",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
